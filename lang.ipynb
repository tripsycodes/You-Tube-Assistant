{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(\"try.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://blog.google/technology/ai/google-gemini-ai/#sundar-note\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(URL)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text from the website data document\n",
    "text_content = docs[0].page_content\n",
    "\n",
    "# The text content between the substrings \"code, audio, image and video.\" to\n",
    "# \"Cloud TPU v5p\" is relevant for this tutorial. You can use Python's `split()`\n",
    "# to select the required content.\n",
    "if(len(text_content.split(\"code, audio, image and video.\",1)) > 1):\n",
    "    text_content_1 = text_content.split(\"code, audio, image and video.\",1)[1]\n",
    "    final_text = text_content_1.split(\"Cloud TPU v5p\",1)[0]\n",
    "\n",
    "else:\n",
    "    final_text = text_content.split(\"Cloud TPU v5p\",1)[0]\n",
    "\n",
    "# Convert the text to LangChain's `Document` format\n",
    "docs =  [Document(page_content=final_text, metadata={\"source\": \"local\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "                     documents=docs,                 # Data\n",
    "                     embedding=embeddings,    # Embedding model\n",
    "                     persist_directory=\"./chroma_db\" # Directory to save data\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tript\\AppData\\Local\\Temp\\ipykernel_10892\\883099469.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore_disk = Chroma(\n",
      "C:\\Users\\tript\\AppData\\Local\\Temp\\ipykernel_10892\\883099469.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(len(retriever.get_relevant_documents(\"MMLU\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Load from disk\n",
    "vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"./chroma_db\",       # Directory of db\n",
    "                        embedding_function=embeddings   # Embedding model\n",
    "                   )\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})\n",
    "print(len(retriever.get_relevant_documents(\"MMLU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model = \"gemini-pro\",temperature = 0.75, top_p = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} template=\"You are an assistant for question-answering tasks.\\nUse the following context to answer the question.\\nIf you don't know the answer, then refer to your own intelligence.\\nUse five sentences maximum and keep the answer concise.\\n\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, then refer to your own intelligence.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = input(\"What question do you have in your mind on reading it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pegasus is a mythical winged horse in Greek mythology, often depicted as pure white in color. It was sired by Poseidon, the god of the sea, and foaled by the Gorgon Medusa.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} template='Write a concise summary of the following:\\n\"{text}\"\\nCONCISE SUMMARY:'\n"
     ]
    }
   ],
   "source": [
    "doc_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
    "\n",
    "# To query Gemini\n",
    "template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_chain = (\n",
    "    # Extract data from the documents and add to the key `text`.\n",
    "    {\n",
    "        \"text\": lambda docs: \"\\n\\n\".join(\n",
    "            format_document(doc, doc_prompt) for doc in docs\n",
    "        )\n",
    "    }\n",
    "    | prompt         # Prompt for Gemini\n",
    "    | llm                # Gemini function\n",
    "    | StrOutputParser()  # output parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Google's Gemini, a large-scale AI model, surpasses state-of-the-art performance on various benchmarks, including language understanding, reasoning, coding, and competitive programming. Gemini's native multimodality enables it to seamlessly understand and reason across different inputs, including text, images, and audio. It excels in sophisticated reasoning, extracting insights from complex data, and explaining reasoning in subjects like math and physics. Gemini's scalability, efficiency, and reliability make it suitable for various applications in fields such as science, finance, and software development.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff_chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_agent():\n",
    "    llm = ChatGoogleGenerativeAI(model = \"gemini-pro\",temperature = 0.75, top_p = 0.85)\n",
    "    tools = load_tools([ \"wikipedia\", \"llm-math\"], llm = llm)\n",
    "    agent = initialize_agent(\n",
    "        tools,llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True\n",
    "    )\n",
    "  \n",
    "    \n",
    "    result = agent.run(\"What is GenAI?\")\n",
    "    \n",
    "    print(result)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tript\\AppData\\Local\\Temp\\ipykernel_10892\\1314820065.py:4: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
      "  agent = initialize_agent(\n",
      "C:\\Users\\tript\\AppData\\Local\\Temp\\ipykernel_10892\\1314820065.py:9: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = agent.run(\"What is GenAI?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI do not know what GenAI is, but I can search the web for it.\n",
      "Action: wikipedia\n",
      "Action Input: What is GenAI?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Generative artificial intelligence\n",
      "Summary: Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models often generate output in response to specific prompts. Generative AI systems learn the underlying patterns and structures of their training data, enabling them to create new data. \n",
      "Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\n",
      "Generative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.\n",
      "\n",
      "Page: Generation Alpha\n",
      "Summary: Generation Alpha (often shortened to Gen Alpha) is the demographic cohort succeeding Generation Z. Researchers and popular media use the early 2010s as starting birth years to the mid-2020s as the ending birth years (see ยง Date and age range definitions). Named after alpha, the first letter in the Greek alphabet, Generation Alpha is the first to be born entirely in the 21st century and the third millennium. The majority of Generation Alpha are the children of Millennials. \n",
      "Generation Alpha has been born at a time of falling fertility rates across much of the world, and experienced the effects of the COVID-19 pandemic as young children. For those with access, children's entertainment has been increasingly dominated by electronic technology, social networks, and streaming services, with interest in traditional television concurrently falling. Changes in the use of technology in classrooms and other aspects of life have had a significant effect on how this generation has experienced early learning compared to previous generations. Studies have suggested that health problems related to screen time, allergies, and obesity became increasingly prevalent in the late 2010s.\n",
      "\n",
      "\n",
      "\n",
      "Page: ServiceNow\n",
      "Summary: ServiceNow, Inc. is an American software company based in Santa Clara, California, that develops a cloud computing platform to help companies manage digital workflows for enterprise operations. Founded in 2003 by Fred Luddy, ServiceNow is listed on the New York Stock Exchange and is a constituent of the Russell 1000 Index and S&P 500 Index. In 2018, Forbes magazine named it number one on its list of the world's most innovative companies.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mBased on the information I have found, GenAI refers to Generative artificial intelligence (generative AI, GenAI, or GAI).\n",
      "Final Answer: Generative artificial intelligence (generative AI, GenAI, or GAI)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Generative artificial intelligence (generative AI, GenAI, or GAI)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    langchain_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
